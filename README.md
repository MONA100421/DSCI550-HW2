# ğŸ•¯ï¸ TEAM03 - DSCI 550: Haunted Places â€“ Large Scale Data Extraction & Analysis

## 1. ğŸ§­ Overview

This project extends Homework 1 on Haunted Places by implementing **large-scale data enrichment and multimedia integration** using NLP, computer vision, geospatial services, and generative AI. Our objective was to transform raw descriptions of haunted places into a richly augmented dataset that includes:

- ğŸŒ **Geolocated coordinates** from text using Apache Tika's GeoTopicParser  
- ğŸ§  **Named Entity Recognition (NER)** from the story using SpaCy  
- ğŸ¨ **AI-generated images** using Stable Diffusion v1.5 based on text prompts  
- ğŸ–¼ï¸ **Image captioning** and ğŸ·ï¸ **object recognition** using Tika's Im2Txt REST container  
- ğŸ“¦ A final dataset integrating all features in a structured, tab-separated format  

The final dataset (`haunted_places_final_v2.tsv`) contains **10,974 rows** with multi-modal annotations.

## 2. ğŸ“ Folder Structure

```bash
Data/
â”œâ”€â”€ haunted_places_v1.tsv              # Original dataset
â”œâ”€â”€ haunted_places_v2.tsv              # With AI image paths
â”œâ”€â”€ haunted_places_v2_with_objects.tsv # With detected objects
â”œâ”€â”€ haunted_places_geoparsed.csv       # With GeoTopicParser lat/lon
â”œâ”€â”€ merged_data_v2_with_entities.tsv   # With NER entities
â”œâ”€â”€ haunted_places_final_v2.tsv        # Final merged dataset

Source Code/
â”œâ”€â”€ add_ai_image_paths.py              # Adds AI-generated image filenames
â”œâ”€â”€ add_entities.py                    # Adds captions/objects using Tika REST
â”œâ”€â”€ entity_extraction.ipynb            # SpaCy NER on descriptions
â”œâ”€â”€ geolocation_extraction.ipynb       # GeoTopicParser-based lat/lon
â”œâ”€â”€ image_captioning.ipynb             # Image captioning (Docker Tika)
â”œâ”€â”€ image_captioning.py                # Batch version using Tika REST
â”œâ”€â”€ image_generation.py                # Image generation with Stable Diffusion
â”œâ”€â”€ named_entity_recognition.ipynb     # SpaCy pipeline + Named_Entities column
â”œâ”€â”€ update_dataset.ipynb               # Final merge script

Others/
â”œâ”€â”€ Requirements.txt
â”œâ”€â”€ tika-config.xml
â”œâ”€â”€ README.md                          
â”œâ”€â”€ TEAM_03_EXTRACT.pdf                # Final report
```

## 3. ğŸ›  Tools & Libraries Used

### Core Technologies
- **Python 3.10+**
- **Apache Tika (Server + REST Docker)** â€“ OCR, image captioning, object detection
- **SpaCy** â€“ Named Entity Recognition (NER)
- **GeoTopicParser (Lucene Gazetteer)** â€“ Geoparsing for latitude/longitude
- **Stable Diffusion v1.5 (via `diffusers`)** â€“ Text-to-image generation

### Supporting Libraries
- `pandas`, `requests`, `tika`, `os`, `json`, `tqdm`, `subprocess`, `logging`
- All dependencies are listed in `Requirements.txt`

## 4. ğŸš€ How to Run the Project

### ğŸ”§ Step 1: Install dependencies
```bash
pip install -r Requirements.txt
```

### ğŸŒ Step 2: Start GeoTopicParser (Lucene Gazetteer)
Follow setup instructions at:
> https://github.com/chrismattmann/lucene-geo-gazetteer  
Make sure the service runs at `http://localhost:8765/search`.

### ğŸ¨ Step 3: Generate AI images (Stable Diffusion)
```bash
python image_generation.py
```
- Prompts are constructed using `city`, `state`, `description`, `apparition_type`.
- Output saved to `Data/images` and linked via `ai_image_path`.

### ğŸ§  Step 4: Perform NER using SpaCy
```bash
# Option 1: Notebook
named_entity_recognition.ipynb
# Option 2: Python script
python entity_extraction.py
```

### ğŸ“ Step 5: Geocode using GeoTopicParser
```bash
python geolocation_extraction.ipynb
```

### ğŸ–¼ï¸ Step 6: Caption + Detect Objects using Tika Docker
- Start Docker container (image captioning):
```bash
docker run -d -p 9998:9998 --name tika-caption uscdatascience/im2txt-rest-tika
```

- Run:
```bash
python image_captioning.py
```

### ğŸ§© Step 7: Merge All Results
```bash
python update_dataset.ipynb
```
Final file: `Data/haunted_places_final_v2.tsv`

## 5. ğŸ“Œ Technical Notes

- All paths are relative and OS-compatible.
- Docker is **required** for image captioning (via Apache Tika).
- If using Stable Diffusion, ensure you have access to the model weights or HuggingFace account.
- You may need to skip failed records or adjust batch size for large-scale processing.

## 6. ğŸ‘©â€ğŸ’» Team Members

**Team 03**
- ğŸ§  Colin Leahey ([cleahey@usc.edu](mailto:cleahey@usc.edu))  
- ğŸ§  Zili Yang ([ziliy@usc.edu](mailto:ziliy@usc.edu))  
- ğŸ§  Chen Yi Weng ([wengchen@usc.edu](mailto:wengchen@usc.edu))  
- ğŸ§  Aadarsh Sudhir Ghiya ([aadarshs@usc.edu](mailto:aadarshs@usc.edu))  
- ğŸ§  Niromikha Jayakumar ([njayakum@usc.edu](mailto:njayakum@usc.edu))  
- ğŸ§  Yung Yee Chia ([yungyeec@usc.edu](mailto:yungyeec@usc.edu))  